{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhOg4cfFyd1bV43djj5H5U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejasvini2805/160122771017_BDA_ASSIGNMENT_2/blob/main/160122771017_BigDataAnalytics_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download Spark 3.4.1 with Hadoop 3 from an official Apache mirror\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Extract Spark\n",
        "!tar -xzf spark-3.4.1-bin-hadoop3.tgz\n",
        "\n",
        "# Install findspark\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set environment variables and initialize Spark\n",
        "import os\n",
        "import findspark\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "\n",
        "findspark.init()\n"
      ],
      "metadata": {
        "id": "tX2lIHP-3KLh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 Build a Classification Model with Spark with a dataset of your choice"
      ],
      "metadata": {
        "id": "JYxSU6g63EAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install Spark and set up\n",
        "!apt-get install openjdk-11-jdk -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"ClassificationModel\").getOrCreate()\n",
        "\n",
        "# Step 2: Load dataset with headers\n",
        "import pandas as pd\n",
        "\n",
        "# Correct adult dataset URL (without 404)\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "\n",
        "# Column names from UCI\n",
        "columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
        "           \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
        "           \"capital-gain\", \"capital-loss\", \"hours-per-week\", \"native-country\", \"income\"]\n",
        "\n",
        "df_pd = pd.read_csv(url, names=columns, na_values=\" ?\", skipinitialspace=True)\n",
        "df_pd.to_csv(\"adult.csv\", index=False)\n",
        "\n",
        "# Step 3: Load into Spark\n",
        "df = spark.read.csv(\"adult.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Step 4: Drop nulls and clean columns\n",
        "df = df.dropna()\n",
        "for col_name in df.columns:\n",
        "    df = df.withColumnRenamed(col_name, col_name.strip())\n",
        "\n",
        "# Step 5: Preprocessing - Index categorical features\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "categorical_cols = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_Index\") for col in categorical_cols]\n",
        "encoders = [OneHotEncoder(inputCol=col + \"_Index\", outputCol=col + \"_Vec\") for col in categorical_cols]\n",
        "\n",
        "# Label indexer\n",
        "label_indexer = StringIndexer(inputCol=\"income\", outputCol=\"label\")\n",
        "\n",
        "# Assembling all features\n",
        "features = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\", \"hours-per-week\"] + [col + \"_Vec\" for col in categorical_cols]\n",
        "assembler = VectorAssembler(inputCols=features, outputCol=\"features\")\n",
        "\n",
        "# Step 6: Pipeline\n",
        "pipeline = Pipeline(stages=indexers + encoders + [label_indexer, assembler])\n",
        "\n",
        "# Step 7: Prepare final dataset\n",
        "model = pipeline.fit(df)\n",
        "df_prepared = model.transform(df)\n",
        "\n",
        "# Step 8: Train classifier\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "train_data, test_data = df_prepared.randomSplit([0.8, 0.2], seed=42)\n",
        "classifier = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "clf_model = classifier.fit(train_data)\n",
        "predictions = clf_model.transform(test_data)\n",
        "\n",
        "# Step 9: Evaluate\n",
        "evaluator = BinaryClassificationEvaluator()\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Test Accuracy (AUC): {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BO4GWUYe5lcC",
        "outputId": "9d23872d-7146-4678-a45d-16f06e9fbeab"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy (AUC): 0.9064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Evaluators for Accuracy, Precision, Recall, and F1 score\n",
        "evaluator_accuracy = MulticlassClassificationEvaluator(metricName=\"accuracy\", labelCol=\"label\")\n",
        "evaluator_precision = MulticlassClassificationEvaluator(metricName=\"weightedPrecision\", labelCol=\"label\")\n",
        "evaluator_recall = MulticlassClassificationEvaluator(metricName=\"weightedRecall\", labelCol=\"label\")\n",
        "evaluator_f1 = MulticlassClassificationEvaluator(metricName=\"f1\", labelCol=\"label\")\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = evaluator_accuracy.evaluate(predictions)\n",
        "precision = evaluator_precision.evaluate(predictions)\n",
        "recall = evaluator_recall.evaluate(predictions)\n",
        "f1_score = evaluator_f1.evaluate(predictions)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision (Weighted): {precision:.4f}\")\n",
        "print(f\"Recall (Weighted): {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMjVt5O-6TH_",
        "outputId": "4fb119f8-1cfb-444f-b7e5-4886a2adc2eb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8506\n",
            "Precision (Weighted): 0.8443\n",
            "Recall (Weighted): 0.8506\n",
            "F1 Score: 0.8453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 Build a Clustering Model with Spark with a dataset of your choice"
      ],
      "metadata": {
        "id": "WUfUPLaU3WTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"ClusteringModel\").getOrCreate()\n",
        "\n",
        "# Step 2: Load dataset\n",
        "import pandas as pd\n",
        "\n",
        "# URL for Iris dataset (CSV)\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"]\n",
        "\n",
        "# Load the dataset into Pandas and then save it as a CSV\n",
        "df = pd.read_csv(url, names=columns)\n",
        "df.to_csv(\"iris.csv\", index=False)\n",
        "\n",
        "# Step 3: Load into Spark\n",
        "spark_df = spark.read.csv(\"iris.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Step 4: Data Preprocessing - Select relevant features\n",
        "# We will exclude the 'class' column for clustering\n",
        "df_features = spark_df.select(\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\")\n",
        "\n",
        "# Step 5: Assemble features into a single vector column\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"], outputCol=\"features\")\n",
        "df_assembled = assembler.transform(df_features)\n",
        "\n",
        "# Step 6: Apply KMeans clustering\n",
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Define the KMeans model with 3 clusters (since Iris dataset has 3 species)\n",
        "kmeans = KMeans(k=3, seed=1, featuresCol=\"features\", predictionCol=\"prediction\")\n",
        "\n",
        "# Train the model\n",
        "model = kmeans.fit(df_assembled)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(df_assembled)\n",
        "\n",
        "# Step 7: Evaluate the model - Evaluate clustering using the training cost (similar to WSSSE)\n",
        "wssse = model.summary.trainingCost\n",
        "print(f\"Within Set Sum of Squared Errors (WSSSE): {wssse}\")\n",
        "\n",
        "\n",
        "# Step 8: Show the clusters\n",
        "predictions.select(\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"prediction\").show(10)\n",
        "\n",
        "# Step 9: Show the cluster centers\n",
        "centers = model.clusterCenters()\n",
        "print(\"Cluster Centers: \")\n",
        "for center in centers:\n",
        "    print(center)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBEIU-ne3ij7",
        "outputId": "21deda5f-b619-4e6b-98e8-56579d4fdc7d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Within Set Sum of Squared Errors (WSSSE): 78.94084142614598\n",
            "+------------+-----------+------------+-----------+----------+\n",
            "|sepal_length|sepal_width|petal_length|petal_width|prediction|\n",
            "+------------+-----------+------------+-----------+----------+\n",
            "|         5.1|        3.5|         1.4|        0.2|         1|\n",
            "|         4.9|        3.0|         1.4|        0.2|         1|\n",
            "|         4.7|        3.2|         1.3|        0.2|         1|\n",
            "|         4.6|        3.1|         1.5|        0.2|         1|\n",
            "|         5.0|        3.6|         1.4|        0.2|         1|\n",
            "|         5.4|        3.9|         1.7|        0.4|         1|\n",
            "|         4.6|        3.4|         1.4|        0.3|         1|\n",
            "|         5.0|        3.4|         1.5|        0.2|         1|\n",
            "|         4.4|        2.9|         1.4|        0.2|         1|\n",
            "|         4.9|        3.1|         1.5|        0.1|         1|\n",
            "+------------+-----------+------------+-----------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "Cluster Centers: \n",
            "[5.9016129  2.7483871  4.39354839 1.43387097]\n",
            "[5.006 3.418 1.464 0.244]\n",
            "[6.85       3.07368421 5.74210526 2.07105263]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 Build a Recommendation Engine with Spark with a dataset of your\n",
        "choice"
      ],
      "metadata": {
        "id": "ORoLEGf77AJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEK8Qknj9G7J",
        "outputId": "aead6018-b48f-412c-8638-7868a4153c16"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "# Step 1: Initialize Spark Session\n",
        "spark = SparkSession.builder.master(\"local\").appName(\"MovieRecommendation\").getOrCreate()\n",
        "\n",
        "# Step 2: Download the MovieLens 100k dataset from a URL (it is hosted online)\n",
        "url = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
        "filename = \"/tmp/ml-100k.zip\"\n",
        "urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "# Step 3: Extract the zip file\n",
        "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/tmp\")\n",
        "\n",
        "# Step 4: Load the ratings data into a DataFrame\n",
        "ratings_file_path = \"/tmp/ml-100k/u.data\"\n",
        "ratings_df = spark.read.option(\"delimiter\", \"\\t\").csv(ratings_file_path, header=False, inferSchema=True)\n",
        "\n",
        "# Step 5: Rename columns for better readability\n",
        "ratings_df = ratings_df.withColumnRenamed(\"_c0\", \"user_id\") \\\n",
        "                       .withColumnRenamed(\"_c1\", \"item_id\") \\\n",
        "                       .withColumnRenamed(\"_c2\", \"rating\") \\\n",
        "                       .withColumnRenamed(\"_c3\", \"timestamp\")\n",
        "\n",
        "# Step 6: Show the first few rows of the dataset\n",
        "ratings_df.show(5)\n",
        "\n",
        "# Step 7: Set up the ALS model for collaborative filtering\n",
        "# Adjust ALS parameters, including regularization and rank (hyperparameters)\n",
        "als = ALS(userCol=\"user_id\", itemCol=\"item_id\", ratingCol=\"rating\",\n",
        "          nonnegative=True, implicitPrefs=False, rank=50, maxIter=10, regParam=0.1, coldStartStrategy=\"drop\")\n",
        "\n",
        "# Step 8: Split the data into training and test sets\n",
        "(training_data, test_data) = ratings_df.randomSplit([0.8, 0.2], seed=1234)\n",
        "\n",
        "# Step 9: Fit the ALS model to the training data\n",
        "als_model = als.fit(training_data)\n",
        "\n",
        "# Step 10: Make predictions on the test data\n",
        "predictions = als_model.transform(test_data)\n",
        "\n",
        "# Step 11: Ensure no null values in the predictions\n",
        "predictions_filtered = predictions.filter(predictions[\"prediction\"].isNotNull())\n",
        "\n",
        "# Step 12: Show some of the predictions\n",
        "predictions_filtered.show(5)\n",
        "\n",
        "# Step 13: Calculate RMSE to evaluate the model\n",
        "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions_filtered)\n",
        "print(f\"Root-Mean-Square Error (RMSE) on test data = {rmse}\")\n",
        "\n",
        "# Step 14: Generate top 10 movie recommendations for each user\n",
        "user_recommendations = als_model.recommendForAllUsers(10)\n",
        "user_recommendations.show(5)\n",
        "\n",
        "# Step 15: Generate top 10 user recommendations for each movie\n",
        "movie_recommendations = als_model.recommendForAllItems(10)\n",
        "movie_recommendations.show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56gp1hbmBkzZ",
        "outputId": "135c5b60-31c1-4fca-b423-8b56edf24ccb"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+------+---------+\n",
            "|user_id|item_id|rating|timestamp|\n",
            "+-------+-------+------+---------+\n",
            "|    196|    242|     3|881250949|\n",
            "|    186|    302|     3|891717742|\n",
            "|     22|    377|     1|878887116|\n",
            "|    244|     51|     2|880606923|\n",
            "|    166|    346|     1|886397596|\n",
            "+-------+-------+------+---------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------+-------+------+---------+----------+\n",
            "|user_id|item_id|rating|timestamp|prediction|\n",
            "+-------+-------+------+---------+----------+\n",
            "|    148|     70|     5|877021271| 3.1155865|\n",
            "|    148|     71|     5|877019251| 3.6562562|\n",
            "|    148|     89|     5|877398587| 3.9636848|\n",
            "|    148|    114|     5|877016735| 4.4826035|\n",
            "|    148|    177|     2|877020715|  3.580373|\n",
            "+-------+-------+------+---------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Root-Mean-Square Error (RMSE) on test data = 0.9119366885453809\n",
            "+-------+--------------------+\n",
            "|user_id|     recommendations|\n",
            "+-------+--------------------+\n",
            "|      1|[{169, 4.9104013}...|\n",
            "|      3|[{320, 4.5112166}...|\n",
            "|      5|[{169, 4.5483036}...|\n",
            "|      6|[{1463, 4.945806}...|\n",
            "|      9|[{1463, 5.1138062...|\n",
            "+-------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------+--------------------+\n",
            "|item_id|     recommendations|\n",
            "+-------+--------------------+\n",
            "|      1|[{357, 5.1376185}...|\n",
            "|      3|[{472, 4.396308},...|\n",
            "|      5|[{688, 4.676782},...|\n",
            "|      6|[{174, 4.7120605}...|\n",
            "|      9|[{592, 4.8394656}...|\n",
            "+-------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}